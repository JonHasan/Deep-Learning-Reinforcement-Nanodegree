{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Navigation 1\n",
    "\n",
    "## Utilize Deep-Q Network to teach agent to navigate an environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was made to train an agent to successfully navigate a world filled with bananas. The picture below shows an example of an agent trying to navigate it. \n",
    "\n",
    "![agent](img/banana.gif)\n",
    "\n",
    "The state space has 37 dimensions. One for velocity and the rest being for how close objects (in this case bananas) are to the forward direction of the agent. \n",
    "\n",
    "A picture of the state space is shown below. \n",
    "\n",
    "![StateSpace](img/StateSpace.PNG))\n",
    "\n",
    "The action space has 4 dimensions. These are the directions the agent can go to maximize its reward. These are forward, backward, left and right. \n",
    "\n",
    "![ActionSpace](img/ActionSpace.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part two: The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is an episodic task. Episodic meaning that there is a finite goal. The goal is to acquire an average score of 13.0. The algorithm we need has to maximize the expected cumulative reward. \n",
    "\n",
    "The algorithm that is needed for this is the algorithm called Deep Q-learning. \n",
    "\n",
    "![Q_learning](img/Qlearning.png)\n",
    "\n",
    "source: https://developer.ibm.com/articles/cc-reinforcement-learning-train-software-agent/\n",
    "\n",
    "It is time to delve into details. The Deep Q learning algorithm was made as a cutting edge algorithm for expanding reinforcment learning into higher dimensional spaces. Previously, the spaces had to be constantly monitored and low dimensional. With neural networks however, reinforcement learning has now seen improved performance in complex environments. \n",
    "\n",
    "The Q value for a state and action is updated depending on a learning rate, $\\alpha$, and the best action resulting from following the policy defined by the agent. \n",
    "\n",
    "The $\\gamma$ term is known as the discount factor and is responsible for determining the contribution of future rewards. If the value is high, future rewards are taken into account by the algorithm. If the value is low, only immediate rewards are important. \n",
    "\n",
    "After this is done, the weights need to be updated. In regular Q learning, \" a guess is updated with a guess\". This can lead to bad correlations and so the update rule that is needed is shown below. The update rule keeps the weights fixed for generating targets and prevents oscillations from happening. The weights are then updated for the initial Q state for some learning steps and then the static w updates again. \n",
    "\n",
    "![update](img/UpdateRule.PNG)\n",
    "\n",
    "The network used in this project is a two layer network with 64 hidden units in each layer. The input layer has 37 units for the state space and 4 output units for the action space. \n",
    "\n",
    "All of the experiences generated in a neural network will be placed in a replay buffer that can be sampled uniformly in pre-chosen batch sizes. This way the common and uncommon actions can be used by the agent to learn. \n",
    "\n",
    "The Q learning algorithm will use exploration and exploitation to try and maximize its reward while using an epsilon greedy policy. The epsilon parameter is responsible for having an agent decide whether to explore new actions or to exploit existing knowledge. It starts off close to 1 and then decays to .01. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Results of the Navigation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the Deep Q learning algorithm, the environment was solved in 483 episodes with an average score of 13. This was on CPU only. GPU wasn't required but if used would have converged faster. \n",
    "\n",
    "The plot for the algorithm is shown below. \n",
    "\n",
    "![Q_plot](img/QlearningPlot.PNG)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways that this project could be improved. \n",
    "\n",
    "From the neural network side, hyperparameter tweaks can be applied as well as expanding the neural network to be more complex by adding more hidden layers and increasing the amount of units in those layers. \n",
    "\n",
    "On the other hand, the algorithm itself could be expanded and improved.\n",
    "\n",
    "Q learning tends to overestimate Action Values. This is a result of noisy Q values. Using a Double DQN can mitigate this problem. The double DQN uses a set of weights to choose an action but is then evaluated by a second set of weights to determine if the action is good or not. \n",
    "\n",
    "The second thing is using prioritized experience replay, the original DQN algorithm samples experience transitions uniformly. However, some transitions could be more important than others for learning. The more important transitions should be have a higher probability of being sample. \n",
    "\n",
    "The third and final improvement would be to use a dueling DQN that has two seperate neural networks. One of these networks is for the value and the other is for the advantage of the action.\n",
    "\n",
    "\n",
    "The final frontier would be to combine all of these approaches. Research was done into an algorithm called \"Rainbow\". It combined all three of these approaches and achieved results that beat all previous approaches by themselves.\n",
    "\n",
    "The paper is titled \"Rainbow: Combining Improvements in Reinforcement Learning\"\n",
    "https://arxiv.org/abs/1710.02298\n",
    "\n",
    "The below picture is taken from figure 1 in that paper and shows how rainbow compares to previous advances. \n",
    "\n",
    "![rainbow](img/RainbowFigure.PNG)\n",
    "\n",
    "Rainbow completely blows everything else out of the water. I would like to see if I could learn to implement this but I have to walk before learning to run. The double DQN followed by the Dueling DQN would be the improvements I would try and implement.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

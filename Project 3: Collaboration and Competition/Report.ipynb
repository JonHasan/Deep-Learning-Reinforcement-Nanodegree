{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for the Collaboration Reinforcement Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment \n",
    "\n",
    "In this project, one will work with the Tennis Environment \n",
    "\n",
    "![tennis](img/tennis.png)\n",
    "\n",
    "In this environment, two agents are trying to cooperate to ensure that the ball doesn't hit the ground. If an agent hits the ball over the net, the reward is +0.1. If an agent lets the ball hit the ground or hits it out of bounds, it receives a reward of -0.01. \n",
    "\n",
    "Observation space has 8 variables that correspond to the position, velocity of the ball and racket. Each of the agent receives its own observation. The agent can then respond by moving towards or away from the net. \n",
    "\n",
    "The task is episodic. In order for the environment to be considered solved, the average score has to be +0.5. (over 100 consecutive episodes, after taking the max over both agents.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "This project reuses the DDPG algorithm from the project 2: Continuous Control. It is an actor critic method that uses two networks to create an optimal policy from the most desired action. Below is a picture detailing this that is taken from the Deep Learning Reinforcement for Enterprise Nanodegree from Udacity.\n",
    "\n",
    "![actor](img/Actor.PNG)\n",
    "\n",
    "This algorithm also notably uses a replay buffer and soft updates. The replay buffer holds the experiences of the agent. The soft updates strategy is the second notable feature of the DDPG algorithm. What this algorithm does is it mixes the weights of the regular network with the target network gradually. So every time step, .01% of the regular network weights are mixed in with the target network weights. This update strategy allows for faster convergence and can be used with other networks that have actor and critic networks.\n",
    "\n",
    "Since this algorithm was useful for the multi agent environment of project 2, it makes sense that it would be useful for project 3 as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results \n",
    "\n",
    "Using the DDPG algorithm, the environment was solved in 198 episodes. It was really slow at the beginning but then started gaining speed. The hyperparameters that were chosen were most likely not the most optimal. Taking time to tweak learning rates and to make changes the neural networks. Below is a picture of the results. \n",
    "\n",
    "![results](img/Results.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "Since this project uses the same algorithm as project 2, it stands to reason that the improvements that can be implemented are also similar. There are four algorithms that could be a source of inspiration for future improvements. These are Trust Region Policy Optimization, Truncated Natural Policy Gradient, Proximal Policy Optimization and Distributed Distributional Deterministic Policy Gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

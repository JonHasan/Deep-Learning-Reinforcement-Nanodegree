{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for the DDPG agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to train an agent to get a score of 30 or higher in the reacher environment. \n",
    "\n",
    "Below is a picture of the agent trying to navigate its environment.\n",
    "\n",
    "![Reacher](img/reacher.gif)\n",
    "\n",
    "\n",
    "This environment houses a double jointed arm that can move towards desired locations. The agent receives a score of +.01 when it stays at a target location. \n",
    "\n",
    "The agent attempts to get as close to the the goal location as possible. \n",
    "\n",
    "\n",
    "Observation space has a length of 33. Each entry corresponds to a variable such as velocity, position, rotation, and angular velocity. \n",
    "\n",
    "Action space has length 4. This action space represents the torques for each joint. \n",
    "\n",
    "\n",
    "There are two versions of the environment. One is for a single agent and the other is a multi-agent environment with 20 agents total. \n",
    "\n",
    "It was recommended to solve the second environment as it would be much easier than the single agent environment. In order for the environment to be considered solved, the score had to be 30 or above averaged over all the agents. An example of a solved plot is shown below. \n",
    "\n",
    "\n",
    "![Example](img/ExamplePlot.PNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, an actor-critic method will be used. Actor-critic methods are basically a combination of value based and policy based methods. This method uses a value based policy to create a baseline. This baseline is then used to reduce the variance of the policy based method. \n",
    "\n",
    "The algorithm that is used here in this project is called DDPG or Deep Deterministic Policy Gradient method. \n",
    "\n",
    "The reason why we need this algorithm is that it handles continuous action spaces well which a DQN cannot. The DQN can only show the likelihood of performing a certain action. It cannot handle choosing the magnitude of an action. For example, if we want to have the action \"jump\" take values between 0 and 100, the DQN doesn't know what to do with \"jump(50)\". \n",
    "\n",
    "This algorithm has two neural networks, the \"Actor\" and \"Critic\" as is expected. The actor tries to come up with the best baseline possible and the critic uses that baseline to evaluate the best policy. \n",
    "\n",
    "Below is a picture taken from a Udacity Video in the Deep Learning Reinforcement Program that illustrates this. \n",
    "\n",
    "![actor-critic](img/Actor.PNG)\n",
    "\n",
    "The algorithm classification for DDPG has been a point of contention for experts. Some people want to classify DDPG as a DQN for continuous action spaces. In the paper, it is introduced as an Actor-Critic method. The actor and critic are a little different this time around. Every time we query the actor network, we want it to output the best believed action. This best believed action is then used as an argument to evaluate the next target value by the critic network. \n",
    "\n",
    "This algorithm also notably uses a replay buffer and soft updates. The replay buffer holds the experiences of the agent. The soft updates strategy is the second notable feature of the DDPG algorithm. What this algorithm does is it mixes the weights of the regular network with the target network gradually. So every time step, .01% of the regular network weights are mixed in with the target network weights. This update strategy allows for faster convergence and can be used with other networks that have actor and critic networks. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the DDPG algorithm with necessary code changes, the environment was solved in 14 episodes. GPU was used and it took quite a while. It took about 10 hours to complete. These results were possible because of gradient clipping, soft updates and a decaying epsilon factor. These variables helped stabilize the network and allowed the high score to be achieved. Below is a picture of the results. \n",
    "\n",
    "![DDPGPlot](img/DDPGPlot.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many improvements that could be made. I could play around with the hyperparameters to try and get the network to converge faster. However, I would have to do this locally since this script takes so much GPU time. \n",
    "\n",
    "What might offer more of a payoff however, is just trying better algorithms for the job. There are four algorithms that could be a source of inspiration for future improvements. These are Trust Region Policy Optimization, Truncated Natural Policy Gradient, Proximal Policy Optimization and Distributed Distributional Deterministic Policy Gradients. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
